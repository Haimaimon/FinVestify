from flask import Flask, request, jsonify
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from flask_cors import CORS
import yfinance as yf
from datetime import datetime
import os
import schedule
import time
from threading import Thread
import requests
 
app = Flask(__name__)
CORS(app)
'''
# 驻拽爪 注转 转 
def load_stock_data(stock_name):
    paths = [
        f"data/etfs/{stock_name}.csv",
        f"data/stocks/{stock_name}.csv"
    ]
    for file_path in paths:
        try:
            data = pd.read_csv(file_path, parse_dates=["Date"])
            print(f"Loaded data from {file_path}")
            print(f"Columns: {data.columns}")  # 驻住转 注转 转
            data['Date'] = pd.to_datetime(data['Date'])
            data.set_index('Date', inplace=True)
            data = clean_data_columns(data)  # 拽  注转
            print(f"Data after cleaning columns: {data.head()}")  # 驻住转 住驻专 砖专转 
            return data
        except FileNotFoundError:
            print(f"File not found: {file_path}")
        except KeyError as e:
            print(f"Error in data structure for {stock_name}: {e}")
    return None

'''
def load_stock_data(stock_name):
    file_path = "datastocks/sp500_stocks.csv"  # 转 拽抓 注  转
    try:
        # 拽专 拽抓
        data = pd.read_csv(file_path, parse_dates=["Date"])
        print(f"Loaded data from {file_path}")
        
        # 住 注 驻 砖 
        if "Symbol" not in data.columns:
            raise KeyError("Column 'Symbol' not found in the data.")
        
        stock_data = data[data["Symbol"] == stock_name]
        if stock_data.empty:
            raise ValueError(f"No data found for stock: {stock_name}")
        
        # 注 转
        stock_data.set_index("Date", inplace=True)
        stock_data.sort_index(inplace=True)
        
        print(f"Filtered data for {stock_name}:")
        print(stock_data.head())  # 驻住转 砖专转 专砖转 拽
        return stock_data

    except FileNotFoundError:
        print(f"File not found: {file_path}")
    except Exception as e:
        print(f"Error loading stock data: {e}")
    return None



def clean_data_columns(data):
    """
    拽  转 住专转 注转 拽转 转 砖专 注 注专 专.
    """
    if isinstance(data.columns, pd.MultiIndex):
        print(f"Cleaning MultiIndex columns: {data.columns}")
        # 拽  注转  转 专拽 NaN
        valid_columns = [col for col in data.columns if not data[col].isnull().all()]
        data = data[valid_columns]
        
        # 砖专 专拽 注 砖转 注转 专转 (专 砖 砖 MultiIndex)
        data.columns = [col[1] if isinstance(col, tuple) and len(col) > 1 else col for col in data.columns]
    else:
        print("Columns are already flat.")
    
    # 拽转 注转 Close
    if 'Close' not in data.columns:
        raise KeyError("No valid 'Close' column found in the data.")
    
    return data

# 驻拽爪 专 转
def normalize_data(data, feature, stock_name=None):
    # 拽转 MultiIndex   专
    if isinstance(data.columns, pd.MultiIndex):
        # 爪专转 砖 注 
        feature_column = (feature, stock_name)
        if feature_column in data.columns:
            print(f"Using MultiIndex column: {feature_column}")
        else:
            raise KeyError(f"Column '{feature_column}' not found in MultiIndex columns: {data.columns}")
    else:
        #   MultiIndex
        feature_column = feature
        if feature_column in data.columns:
            print(f"Using flat column: {feature_column}")
        else:
            raise KeyError(f"Column '{feature_column}' not found in flat columns: {data.columns}")

    # 拽转 注专 注
    if data[feature_column].isnull().all():
        raise ValueError(f"All values in column '{feature_column}' are NaN.")
    
    # 专 转
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data[feature_column].values.reshape(-1, 1))
    return scaled_data, scaler

# 驻拽爪 爪专转 专爪驻 注专 
def create_sequences(data, sequence_length):
    sequences = []
    for i in range(len(data) - sequence_length):
        sequences.append(data[i:i + sequence_length])
    return np.array(sequences)

# 驻拽爪 转  LSTM
def build_lstm_model(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(50, return_sequences=True, input_shape=input_shape),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.LSTM(50, return_sequences=False),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(25),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# 驻拽爪 注 转 
def update_stock_data(stock_name, file_path):
    try:
        # 注 转 转 拽
        existing_data = pd.read_csv(file_path, parse_dates=["Date"])
        if "Date" not in existing_data.columns:
            raise KeyError("Column 'Date' is missing in the existing file.")
        existing_data["Date"] = pd.to_datetime(existing_data["Date"]).dt.tz_localize(None)  # 住专转 专 
        existing_data.set_index("Date", inplace=True)
        last_date = existing_data.index.max()  # 转专 专 拽抓
    except FileNotFoundError:
        existing_data = pd.DataFrame()
        last_date = datetime(2020, 1, 1)  #   拽抓, 转 -2020
    except KeyError as e:
        print(f"Error in existing data for {stock_name}: {e}")
        return

    # 转专 注
    today = datetime.now().strftime('%Y-%m-%d')

    # 专转 转 注 -yfinance
    try:
        stock_data = yf.download(stock_name, start=last_date.strftime('%Y-%m-%d'), end=today)
    except Exception as e:
        print(f"Failed to fetch data for {stock_name}: {e}")
        return

    # 砖专转 转 砖
    if not stock_data.empty:
        stock_data = stock_data[["Open", "High", "Low", "Close", "Adj Close", "Volume"]]
        stock_data.index = pd.to_datetime(stock_data.index).tz_localize(None)  # 住专转 专 
        stock_data.sort_index(inplace=True)

        # 砖 转 砖 拽
        if not existing_data.empty:
            updated_data = pd.concat([existing_data, stock_data])
            updated_data = updated_data[~updated_data.index.duplicated(keep="last")].sort_index()
        else:
            updated_data = stock_data

        # 砖专转 转 注
        updated_data.to_csv(file_path)
        print(f"Data for {stock_name} updated successfully!")
    else:
        print(f"No new data available for {stock_name}.")



# 驻拽爪 注  转 转拽
def update_all_stocks_in_folder(folder_path):
    for file_name in os.listdir(folder_path):
        if file_name.endswith(".csv"):  # 拽 砖专 拽抓 CSV
            stock_name = file_name.replace(".csv", "")
            file_path = os.path.join(folder_path, file_name)
            update_stock_data(stock_name, file_path)

# 驻拽爪 专爪转 砖转 转转
def run_scheduled_tasks():
    schedule.every().day.at("00:00").do(update_all_stocks_in_folder, folder_path="data/stocks")  #   爪转

    while True:
        schedule.run_pending()  # 驻注 转 砖转 砖 爪注 砖 注
        time.sleep(1)  # 转 砖 驻 拽 

# Route  专 
@app.route('/api/predict', methods=['POST'])
def predict_stock():
    data = request.get_json()
    stock_name = data.get('stock_name')
    sequence_length = data.get('sequence_length', 60)

    # 注 转 转
    stock_data = load_stock_data(stock_name)
    if stock_data is None:
        return jsonify({"error": f"Stock data for {stock_name} not found"}), 404
    
    print(f"Loaded stock data for {stock_name}: {stock_data.head(-1)}")  # 驻住转 转 

    # 专 转
    try:
        scaled_data, scaler = normalize_data(stock_data, 'Close', stock_name=stock_name)
        print(f"Scaled data: {scaled_data[:5]}")  # 驻住转 拽 转 专
    except KeyError as e:
        print(f"Normalization error: {e}")
        return jsonify({"error": str(e)}), 400

    # 爪专转 专爪驻
    x = create_sequences(scaled_data, sequence_length)
    y = scaled_data[sequence_length:]

    # 砖  注专 TensorFlow
    x = np.reshape(x, (x.shape[0], x.shape[1], 1))

    # 转 
    model = build_lstm_model((sequence_length, 1))

    #  
    model.fit(x, y, epochs=10, batch_size=32, verbose=1)

    # 转转
    last_sequence = scaled_data[-sequence_length:]
    last_sequence = last_sequence.reshape(1, sequence_length, 1)
    prediction = model.predict(last_sequence)
    predicted_price = scaler.inverse_transform(prediction)[0][0]

    # 驻 return jsonify...
    historical_prices = stock_data["Close"][-sequence_length:].tolist()
    historical_dates = stock_data.index[-sequence_length:].strftime('%Y-%m-%d').tolist()

    return jsonify({
        "predicted_price": round(float(predicted_price), 2),
        "historical_data": list(zip(historical_dates, historical_prices))
    })

# 转 拽抓
DATA_PATH = "datastocks/sp500_companies.csv"

@app.route('/api/stock-details/<string:stock_name>', methods=['GET'])
def get_stock_details(stock_name):
    try:
        data = pd.read_csv(DATA_PATH)
        stock = data[data['Symbol'] == stock_name.upper()]
        if stock.empty:
            return jsonify({"error": f"No details found for stock: {stock_name}"}), 404

        stock_details = stock.iloc[0].to_dict()
        return jsonify(stock_details)
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
@app.route('/api/suggestions', methods=['GET'])
def get_suggestions():
    query = request.args.get('query', '').upper()
    if not query:
        return jsonify([])
    
    data = pd.read_csv(DATA_PATH)
    suggestions = data[data['Symbol'].str.startswith(query)][['Symbol', 'Longname','Sector']].head(10).to_dict(orient='records')
    return jsonify(suggestions)
 

#  API Key for Polygon.io (Replace with your own API key)
POLYGON_API_KEY = "QK_TtkEeZYSntEOQwX5YeSfNwhbsYtfd"

@app.route('/api/stock-extended/<string:stock_name>', methods=['GET'])
def get_extended_stock_details(stock_name):
    try:
        print(f"Fetching extended data for stock: {stock_name}")

        def fetch_json(url):
            """驻拽爪  砖驻转 JSON -API"""
            response = requests.get(url)
            try:
                return response.json()
            except ValueError:
                print(f"锔 Warning: Invalid JSON response from {url}")
                return None  # 专 None  JSON  转拽祝

        #  1. 52-Week High & Low
        high_low_url = f"https://api.polygon.io/v2/aggs/ticker/{stock_name}/range/1/day/2023-02-20/2024-02-20?apiKey={POLYGON_API_KEY}"
        high_low_data = fetch_json(high_low_url)

        if high_low_data and "results" in high_low_data:
            high_52w = max([item.get("h", 0) for item in high_low_data["results"]])
            low_52w = min([item.get("l", 0) for item in high_low_data["results"]])
        else:
            high_52w, low_52w = "N/A", "N/A"

        #  2. Market Trend Data (e.g., SMA, volatility)
        volatility_url = f"https://api.polygon.io/v2/reference/technical_indicators/volatility/{stock_name}?apiKey={POLYGON_API_KEY}"
        volatility_data = fetch_json(volatility_url)
        volatility = volatility_data.get("volatility", "N/A") if volatility_data else "N/A"

        #  3. Earnings Date
        earnings_url = f"https://api.polygon.io/vX/reference/earnings?ticker={stock_name}&apiKey={POLYGON_API_KEY}"
        earnings_data = fetch_json(earnings_url)
        earnings_date = earnings_data["results"][0]["reportDate"] if earnings_data and "results" in earnings_data else "N/A"

        #  Compile all data into a response
        stock_extended_details = {
            "symbol": stock_name.upper(),
            "high_52w": high_52w,
            "low_52w": low_52w,
            "volatility": volatility,
            "earnings_date": earnings_date,
            "market_trend": "Uptrend" if high_52w != "N/A" and high_52w > low_52w else "Downtrend"
        }

        return jsonify(stock_extended_details)

    except Exception as e:
        print(f"锔 Server Error: {e}")
        return jsonify({"error": str(e)}), 500

   
if __name__ == '__main__':
    # 驻注转 注  砖  转
    #print("Starting initial update of all stocks...")
    #update_all_stocks_in_folder("data/stocks")

    # 驻注转 注 转 专拽注
    update_thread = Thread(target=run_scheduled_tasks)
    update_thread.daemon = True
    update_thread.start()

    # 驻注转 Flask
    app.run(port=5658, debug=True)
